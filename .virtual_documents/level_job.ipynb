import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import re
from sklearn.utils.class_weight import compute_class_weight
import xgboost as xgb
from sklearn.utils.class_weight import compute_sample_weight



df = pd.read_csv('data_science_job.csv', 
                 encoding='latin1')


df


print(df.shape)
print(df.info())
print(df.describe(include='all'))
print(df.isnull().sum())


# Strip whitespace dari kolom string
str_cols = df.select_dtypes(include='object').columns
df[str_cols] = df[str_cols].apply(lambda col: col.str.strip())

# Bersihkan kolom Salary: buang simbol, konversi ke numerik (ambil nilai minimum)
df['Salary_clean'] = df['Salary'].apply(lambda x: float(re.findall(r'\d+', x.replace(',', ''))[0]) if pd.notnull(x) else np.nan)

# Ubah Experience level jadi lowercase & standardize
df['Experience level'] = df['Experience level'].str.lower().str.strip()

# Isi missing Experience level dengan 'unknown'
df['Experience level'] = df['Experience level'].fillna('unknown')


print(df.shape)
print(df.info())
print(df.describe(include='all'))
print(df.isnull().sum())


print(df.describe(include=[np.number]))
print(df.describe(include=[object]))


df['Facilities_list'] = df['Facilities'].apply(lambda x: [f.strip() for f in str(x).split(',') if f.strip()])
df['Country'] = df['Location'].apply(lambda x: x.split(',')[-1].strip() if pd.notnull(x) else 'unknown')


df['Num_facilities'] = df['Facilities_list'].apply(len)
df


plt.figure(figsize=(6,4))
sns.countplot(x='Num_facilities', data=df, order=sorted(df['Num_facilities'].unique()))
plt.title('Jumlah Fasilitas per Lowongan')
plt.show()


top_countries = df['Country'].value_counts().head(10)

plt.figure(figsize=(8,4))
sns.barplot(y=top_countries.index, x=top_countries.values)
plt.title('Top 10 Negara dengan Lowongan Terbanyak')
plt.xlabel('Jumlah Lowongan')
plt.show()


plt.figure(figsize=(8,4))
sns.boxplot(x='Experience level', y='Salary_clean', data=df)
plt.title('Distribusi Gaji berdasarkan Experience Level')
plt.show()


all_reqs = ' '.join(df['Requirment of the company '].dropna().astype(str))
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reqs)

plt.figure(figsize=(10,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud Requirment of the company')
plt.show()


top_jobs = df['Job Title'].value_counts().head(10)

plt.figure(figsize=(8,4))
sns.barplot(y=top_jobs.index, x=top_jobs.values)
plt.title('Top 10 Job Title Paling Banyak Dibuka')
plt.xlabel('Jumlah Lowongan')
plt.show()


df = df[df['Experience level'] != 'unknown']
df['Experience level'] = df['Experience level'].str.lower().str.strip()
df


df['Country'] = df['Location'].apply(lambda x: x.split(',')[-1].strip() if pd.notnull(x) else 'unknown')
df['Num_facilities'] = df['Facilities'].apply(lambda x: len([f for f in str(x).split(',') if f.strip()]))
df


df['Salary_clean'] = df['Salary_clean'].fillna(df['Salary_clean'].median())
df


for col in ['Company', 'Country']:
    freq = df[col].value_counts() / len(df)
    df[f'{col}_freq'] = df[col].map(freq)


df['JobTitle_len'] = df['Job Title'].astype(str).apply(len)
df['Req_words_count'] = df['Requirment of the company '].astype(str).apply(lambda x: len(x.split(',')))
df['Num_facilities'] = df['Facilities_list'].apply(len)


df['mean_salary_by_country'] = df.groupby('Country')['Salary_clean'].transform('mean')
df['mean_salary_by_company'] = df.groupby('Company')['Salary_clean'].transform('mean')


df['salary_minus_country_mean'] = df['Salary_clean'] - df['mean_salary_by_country']
df['salary_minus_company_mean'] = df['Salary_clean'] - df['mean_salary_by_company']


df = pd.get_dummies(df, columns=['Job Type'], drop_first=True)


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD


# Job Title
tfidf = TfidfVectorizer(max_features=100)
jobtitle_tfidf = tfidf.fit_transform(df['Job Title'].astype(str))

svd = TruncatedSVD(n_components=5, random_state=42)
jobtitle_svd = svd.fit_transform(jobtitle_tfidf)

# Add to df
for i in range(5):
    df[f'jobtitle_svd_{i}'] = jobtitle_svd[:, i]


for col in ['Company', 'Country']:
    grp = df.groupby(col)['Salary_clean']
    df[f'{col}_std_salary'] = df[col].map(grp.std())
    df[f'{col}_min_salary'] = df[col].map(grp.min())
    df[f'{col}_max_salary'] = df[col].map(grp.max())
    df[f'{col}_count'] = df[col].map(grp.count())


df['salary_div_country_mean'] = df['Salary_clean'] / (df['mean_salary_by_country'] + 1e-3)
df['salary_div_company_mean'] = df['Salary_clean'] / (df['mean_salary_by_company'] + 1e-3)


df['company_freq_x_mean_salary'] = df['Company_freq'] * df['mean_salary_by_company']
df['country_freq_x_mean_salary'] = df['Country_freq'] * df['mean_salary_by_country']


keywords = ['Python', 'AWS', 'SQL', 'Machine Learning', 'Deep Learning']
for kw in keywords:
    df[f'req_has_{kw.lower().replace(" ", "_")}'] = df['Requirment of the company '].astype(str).apply(lambda x: int(kw.lower() in x.lower()))


df['salary_rank_country'] = df.groupby('Country')['Salary_clean'].rank(pct=True)
df['salary_rank_company'] = df.groupby('Company')['Salary_clean'].rank(pct=True)


df['salary_bin'] = pd.qcut(df['Salary_clean'], q=5, duplicates='drop', labels=False)


df['has_career_development'] = df['Facilities_list'].apply(lambda x: int('Career development' in x))
df['num_facilities'] = df['Facilities_list'].apply(len)


df


drop_cols = [
    'Job Title'
    'Location', 'Salary', 'Requirment of the company',
    'Facilities', 'Facilities_list', 'Country'
]


le_company = LabelEncoder()
df['Company_encoded'] = le_company.fit_transform(df['Company'].astype(str))


feature_cols = [
    'Company_encoded',
    'Salary_clean',
    'mean_salary_by_company', 'salary_minus_country_mean', 'salary_minus_company_mean',
    'Job Type_Internship', 'Job Type_Part Time',
    'req_has_python', 'req_has_aws', 'req_has_sql', 'req_has_machine_learning', 'req_has_deep_learning',
    'salary_rank_country', 'salary_rank_company', 'salary_bin', 'has_career_development', 'num_facilities',
    'jobtitle_svd_0', 'jobtitle_svd_1', 'jobtitle_svd_2', 'jobtitle_svd_3', 'jobtitle_svd_4'
]

X = df[feature_cols]
y = df['Experience level']


le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42
)


from sklearn.utils.class_weight import compute_sample_weight


sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)


clf = XGBClassifier(
    n_estimators=200,
    max_depth=7,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    tree_method='hist'
)


clf.fit(
    X_train, y_train,
    sample_weight=sample_weights,
    eval_set=[(X_test, y_test)],
    verbose=10


y_pred = clf.predict(X_test)

print("\nâœ… Accuracy:", accuracy_score(y_test, y_pred))
print("\nâœ… Classification Report:\n", classification_report(
    y_test, y_pred, target_names=le_target.classes_))
print("\nâœ… Confusion Matrix:\n", confusion_matrix(y_test, y_pred))


# Ambil feature importance dari model
importance = clf.feature_importances_

# Buat dataframe agar mudah dibaca
feat_imp = pd.DataFrame({
    'feature': feature_cols,
    'importance': importance
}).sort_values(by='importance', ascending=False)

print("=== Feature Importance ===")
print(feat_imp)



plt.figure(figsize=(10,6))
plt.barh(feat_imp['feature'], feat_imp['importance'])
plt.gca().invert_yaxis()
plt.title('Feature Importance')
plt.show()



from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier


threshold = 0.013
selected_features = feat_imp[feat_imp['importance'] >= threshold]['feature'].tolist()

print(f"\nâœ… Selected features (len={len(selected_features)}):\n{selected_features}")


preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), selected_features)
    ]
)


xgb_clf = XGBClassifier(
    objective='multi:softmax',
    num_class=len(le_target.classes_),
    eval_metric='mlogloss',
    use_label_encoder=False,
    booster='dart',         # ðŸ§ª Eksperimen: 'gbtree' (default), 'dart', 'gblinear'
    reg_alpha=0.1,         # âœ… Regularization L1
    reg_lambda=1.0,        # âœ… Regularization L2
    class_weight='balanced', # kalau imbalance
    random_state=42
)


pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', xgb_clf)
])


pipeline.fit(X_train[selected_features], y_train)


y_pred = pipeline.predict(X_test[selected_features])

print("\nâœ… Accuracy:", accuracy_score(y_test, y_pred))
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred, target_names=le_target.classes_))
print("\nâœ… Confusion Matrix:\n", confusion_matrix(y_test, y_pred))



